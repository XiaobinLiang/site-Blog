<!DOCTYPE HTML>
<html lang="zh-CN">

<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="renderer" content="webkit">
    <meta name="HandheldFriendly" content="true">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
    <meta name="keywords" content="Maverick,熊猫小A,Galileo,blog" />
    <meta name="generator" content="Maverick 1.1" />
    <meta name="template" content="Prism" />
    <link rel="alternate" type="application/rss+xml" title="无文字 | 三无计划 &raquo; RSS 2.0" href="/feed/index.xml" />
    <link rel="alternate" type="application/atom+xml" title="无文字 | 三无计划 &raquo; ATOM 1.0" href="/feed/atom/index.xml" />
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/AlanDecode/site-Blog@gh-pages/assets/prism-561767ec37.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/AlanDecode/site-Blog@gh-pages/assets/ExSearch/ExSearch-182e5a8869.css">
    <link href="https://fonts.googleapis.com/css?family=Fira+Code&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@4.7.0/css/font-awesome.min.css">
    <script>
        var ExSearchConfig = {
            root: "",
            api: "https://cdn.jsdelivr.net/gh/AlanDecode/site-Blog@gh-pages/993aa3dcb70943b3e506b13b3d502541.json"
        }

    </script>
    
<title>无文字 | 三无计划</title>
<meta name="author" content="熊猫小A" />
<meta name="description" content="只坚持一种正义。我的正义。" />
<meta property="og:title" content="无文字 | 三无计划" />
<meta property="og:description" content="只坚持一种正义。我的正义。" />
<meta property="og:site_name" content="无文字 | 三无计划" />
<meta property="og:type" content="website" />
<meta property="og:url" content="/page/7/" />
<meta property="og:image" content="无文字 | 三无计划" />
<meta name="twitter:title" content="无文字 | 三无计划" />
<meta name="twitter:description" content="只坚持一种正义。我的正义。" />
<meta name="twitter:card" content="summary" />
<meta name="twitter:image" content="https://cdn.jsdelivr.net/gh/AlanDecode/site-Blog@gh-pages/android-chrome-512x512.png" />


    
<meta http-equiv="x-dns-prefetch-control" content="on">
<link rel="dns-prefetch" href="//cdn.jsdelivr.net" />
<link rel="dns-prefetch" href="//blog.imalan.cn" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/AlanDecode/site-Blog@gh-pages/brand_font/embed.css" />
<style>.brand{font-family:FZCuJinLFW,serif;font-weight: normal!important;}</style>
<meta name="apple-mobile-web-app-capable" content="yes">
<meta name="apple-mobile-web-app-status-bar-style" content="black">
<link rel="apple-touch-icon" sizes="180x180" href="https://cdn.jsdelivr.net/gh/AlanDecode/site-Blog@gh-pages/apple-touch-icon.png?v=PY43YeeEKx">
<link rel="icon" type="image/png" sizes="32x32" href="https://cdn.jsdelivr.net/gh/AlanDecode/site-Blog@gh-pages/favicon-32x32.png?v=yyLyaqbyRG">
<link rel="icon" type="image/png" sizes="16x16" href="https://cdn.jsdelivr.net/gh/AlanDecode/site-Blog@gh-pages/favicon-16x16.png?v=yyLyaqbyRG">
<link rel="mask-icon" href="https://cdn.jsdelivr.net/gh/AlanDecode/site-Blog@gh-pages/safari-pinned-tab.svg?v=yyLyaqbyRG" color="#505050">
<link rel="shortcut icon" href="https://cdn.jsdelivr.net/gh/AlanDecode/site-Blog@gh-pages/favicon.ico?v=yyLyaqbyRG">
<meta name="application-name" content="三無計劃">
<meta name="apple-mobile-web-app-title" content="三無計劃">
<meta name="msapplication-TileColor" content="#000000">
<meta name="theme-color" content="#000000">
<meta name="baidu-site-verification" content="9BEwwo6Ibg" />

</head>

<body>
    <div class="container prism-container">
        <header class="prism-header" id="prism__header">
            <h1 class="text-uppercase"><a class="no-link" href="/" target="_self">无文字 | 三无计划</a></h1>
            <p>只坚持一种正义。我的正义。</p>
            <nav class="prism-nav"><ul><li><a class="no-link text-uppercase " href="/" target="_self">首页</a></li><li><a class="no-link text-uppercase " href="/archives/" target="_self">归档</a></li><li><a class="no-link text-uppercase " href="/links/" target="_self">友链</a></li><li><a class="no-link text-uppercase " href="/about/" target="_self">关于</a></li><li><a href="#" target="_self" class="search-form-input no-link text-uppercase">搜索</a></li></ul></nav>
        </header>
        <div class="prism-wrapper" id="prism__wrapper">
            
<main>    
    

<section id="prism__post-list" class="prism-section row">
    
    <article class="yue prism-post-item col-md-8 offset-md-2">
        <h1 class="prism-post-title"><a class="no-link" href="/archives/128/" target="_self">熊猫追番 (PandaBangumi) for Typecho 发布！</a></h1>
        <div class="prism-post-time">
            <a class="no-link" href="/archives/128/" target="_self">
                <time class="text-uppercase">
                    May 30 2018
                </time>
            </a>
        </div>
        <div class="prism-content"><p>熊猫追番是一个可以给 Typecho 博客增加番剧展示功能的插件。我之前有写过一个独立页面模板，也是把 Bangumi 上的数据拉过来展示，我觉得也还蛮不错的，但是用起来还是不够方便，且功能不是很完善。这两天鼓捣鼓捣弄了一个插件版出来，用起来应该蛮舒服的～</p>
<h2>更新</h2>
<p>2018-11-18：2.0</p>
<p>移除了大量使用频率低的功能，仅保留追番列表功能。</p>
<ul>
<li>移除了追番日历</li>
<li>移除了公共 CDN</li>
<li>移除了单个番剧展示</li>
<li>无需填写账号与密码，只需 ID</li>
<li>重写样式</li>
</ul>
<p>2018-10-28：开启了精简的版本。在该版本中，去掉了番剧日历与展示单个番剧的功能。如要使用这个版本，请从 Github clone 最新的代码至本地，然后执行 <code>git checkout simple-ver</code>，然后在后台禁用再重新启用插件。新版本不需要账户与密码，只需要填写你的 ID，即你用户主页链接 user 后面那一串数字。</p>
<p>2018-08-29：重新写了样式，现在不只是单个展示的番剧，追番列表中的番剧也可以点击显示简介。请手动清除一下缓存的数据文件（插件设置页有删除缓存的按钮）。移除了并没有什么用的“黑暗模式”。由于增加了简介的缓存，首次加载时间会变长一些……</p>
<h2>简单的功能列表</h2>
<ul>
<li>可展示追番列表与追番进度，带分页功能，可设置每页显示的数量，AJAX 加载</li>
<li><del>可展示番剧播放日历</del></li>
<li><del>可以单独展示某部番剧，一篇文章可以展示多部</del></li>
<li>AJAX 加载 + 数据缓存，用起来应该还满顺畅的</li>
</ul>
<h2>使用</h2>
<p>展示追番列表。</p>
<p>插件版添加了<strong>分页功能</strong>，这样追番很多时能节约流量，加快速度。追番列表与追番日历功能都可以自己选择要不要开启，在插件里设置就好。</p>
<p>使用方法：去 GitHub 上下载插件：</p>
<p><a href="https://github.com/AlanDecode/PandaBangumi-Typecho-Plugin">https://github.com/AlanDecode/PandaBangumi-Typecho-Plugin</a></p>
<p>解压后把文件夹改名为 <code>PandaBangumi</code> ，上传到服务器 <code>/usr/plugins</code> 目录下，在 Typecho 后台启用本插件，填写 ID（即用户主页链接后的那串数字），设置一下每页展示的数量。如果你的博客没有引入 JQuery 的话可以在插件里选择引入。</p>
<p>在任何页面，不论是独立页还是一般的文章页面，在文章里插入这么一句：</p>

<pre><code>&lt;div class="bgm-collection" id="bgm-collection"&gt;&lt;/div&gt;</code></pre>
<p>保存发布，这个位置就会展开成追番展示面板。加载和分页都使用 AJAX 请求～</p>
<p>插件带了缓存功能，可以极大地提升速度，<strong>但是记得要保证 <code>插件目录/json/bangumi.json</code> 这个文件可写</strong>。</p>
<h2>注意事项</h2>
<p>服务器需要启用 PHP curl 扩展。</p>
<p>不一定所有主题都完美。</p>
<p>模板会向 插件目录/json/bangumi.json 文件写入缓存数据，请保证这个文件可写。</p>
<p><strong>如果你发现 PJAX 切换页面时番剧都不加载了，你需要去主题的 PJAX 回调函数中添加一句 initCollection();</strong></p>
<p>如果你的主题没有引入 JQuery，记得在插件设置里选择引入。</p>
<h4>引用与参考的项目</h4>
<p>Bangumi API 部分参考了<a href="http://www.azimiao.com/">梓喵出没</a>大佬写的一篇<a href="http://www.azimiao.com/2768.html">Bangumi的几个API及使用PHP调用的简单测试</a> 。</p>
<p><strong>如果有帮助到你，欢迎在 GitHub 给我 star 哦～</strong></p>
</div>
        
        <div class="prism-action-bar">
            <div class="comment-action action-item"><a class="no-link text-uppercase" href="/archives/128/#prism__comment" target="_self"><i class="fa fa-comment"></i>评论</a></div>
        </div>
        
    </article>
    
    <article class="yue prism-post-item col-md-8 offset-md-2">
        <h1 class="prism-post-title"><a class="no-link" href="/archives/125/" target="_self">一点最近的 Update | 毕业季【2018-05】</a></h1>
        <div class="prism-post-time">
            <a class="no-link" href="/archives/125/" target="_self">
                <time class="text-uppercase">
                    May 15 2018
                </time>
            </a>
        </div>
        <div class="prism-content"><p>一、</p>
<p>从上学期（2017下半年）我搭建这个博客开始，关于页面里的自我介绍就是“即将上研的首都大学生”，如今到了2018年中旬，毕业才真正地临近了。</p>
<p>我的一些不同学校的朋友，许多已经走完了毕业流程，开始了毕业后的生活。毕业旅行也好，找工作进入职场也好，实习也好，大家都开始了人生的一个新阶段。我真心为他/她们感到开心。</p>
<p>我的接下来两年的路径大致已经确定，就是读研，搞科研，精进技术，然后去做一份开发工作。虽然是半路出家的野生程序员，但是对开发的热情渐浓，目前所考虑的路径就如上了。</p>
<p>我不是一个很爱做计划的人，或者不如说，我不是一个很能遵循计划的人。今天早上有位朋友在一条旧微博下面@我，给我看两年前的一张聊天截图：</p>
<p><figure class="pswp-item" style="flex: 28.0859375" data-width="719" data-height="1280"><img src="https://cdn.jsdelivr.net/gh/AlanDecode/site-Blog@gh-pages/archives/assets/c2d860261284e69dae5400d0b09b8222.jpg" alt="photo_2018-05-15_23-36-00" /><figcaption>photo_2018-05-15_23-36-00</figcaption></figure></p>
<p>人生充满了惊喜，这句话永远都是对的。虽说我两年对自己的预言如今多半成真了，但实现过程中零零碎碎的过程还是无法预料，既然无法预料，那就不要想着去掌控一切，做自己觉得正确的事，享受生活给自己的惊喜就好了。</p>
<p>二、</p>
<p>最近，学院要求应届生做一个 presentation，总结大学四年的收获遗憾。我本是不 care 的，知道这是一项政治任务，但是在展示现场，看同班同学一个个上台展示，把一些陈年老照片拿出来展示，还是难免有些感慨。四年的时间，比初中长，比高中长，但却给我留下了远不如初高中深刻的印象。一个侧面展示出我这四年多少还是不够充实吧。</p>
<p>不过从个人成长的角度来说，除了体重从120斤突飞猛进到150斤外，思想境界也有了不错的长进。不是说我脑子里多了多少知识，而是我思考问题的方式在朝我理想中的方向前进，这是最重要的。</p>
<p>一个特点没有改变，就是对感兴趣的事情充满了热情，一定要做到最好，做到自己满意为止。</p>
<p>三、</p>
<p>大一军训把过眉毛的长发（毕竟男生，还挺长的）剪成了不到 5mm 的寸头之后就再也没有搭理过头发的事情，大概是没有了女朋友，也没有太多交际，对自己的外在形象就没那么在意了。如今想要把头发再留回来，遮一下日渐宽广的额头，另外也提醒自己：不过20多岁而已，I still have every reason to be young。</p>
<p>我的头发一旦处于不长不短的状态时就会乱翘，非常不服帖，最关键的是，还只是一部分乱翘、不服帖，另一部分却乖乖的。索性去烫了头，现在所有的头发都乱翘，满意。</p>
<p>胡子也是一周也不刮一次。今天用电动剃须刀试着刮了一下，发现胡子太长，缠住了刀片，扯得疼得不行，只好用刀片手动刮了。</p>
<p>四、</p>
<p>过去四年，浪费了两年。其余两年受了拖累，没有做任何真正酷的东西。如今是一个契机，我希望能够从事真正有趣的工作。没有什么比兴趣更能激发我的潜力。</p>
<p>大概会参与一些开源项目，甚至自己启动一两个也说不定，把浪费的时间补回来。学习手绘，学习日语，谁知道呢？</p>
</div>
        
        <div class="prism-action-bar">
            <div class="comment-action action-item"><a class="no-link text-uppercase" href="/archives/125/#prism__comment" target="_self"><i class="fa fa-comment"></i>评论</a></div>
        </div>
        
    </article>
    
    <article class="yue prism-post-item col-md-8 offset-md-2">
        <h1 class="prism-post-title"><a class="no-link" href="/archives/108/" target="_self">为什么每个人都应该有自己的 Wiki</a></h1>
        <div class="prism-post-time">
            <a class="no-link" href="/archives/108/" target="_self">
                <time class="text-uppercase">
                    April 29 2018
                </time>
            </a>
        </div>
        <div class="prism-content"><p>我们不是生来就具有知识，知识是学习来的、总结来的。</p>
<p>但由于人类大脑的原因，习得的知识并不总是那么稳当，当你需要的时候它们经常消失在大脑里，因此你不得不做无用功，去搜索，再一次习得这部分知识。这个过程周而复始，已经成了一个我觉得只有蠢人才不会在意的问题。</p>
<p>想要让知识在脑海里保持稳固，一个方法是不停地去重复使用、重复学习它，这是无数人提倡多练多用的原因，例如对学习一门语言来讲这种方法不是最轻松的，但是却很稳妥。你可以确信，只要你每天都念同一句话，这一句话总不至于在下一次要说的时候想不起来。</p>
<p>但是这种方法不是所有场景都适合，特别是对某些知识点特别繁杂、但是使用机会却不多的领域。你可以某天都背单词，但我觉得你不愿意每天默写一遍二叉树前中后序遍历的代码，也不愿意每天都温习一遍更改 grub 启动顺序的方法。这类场景和学英语有本质的不同，学英语的目标是融会贯通，内化成自己的一种本能，要求学习者建立起相对完整的知识体系；但我可能一年半载都不会写一次遍历二叉树的代码，这就导致我每天去重复练习它性价比太低。另外，零零散散的知识点何其多，重复练习就是简单地不具有可行性。</p>
<p>对这类知识，最好的办法是招之即来挥之即去，用的时候习得，不用时就不要让它占据自己的大脑太久。这是我推崇印象笔记这类软件的原因。像它声称的一样，要做你的「第二大脑」，意在让人把当下不重要的零散知识 dump 出来，保持大脑的干净整洁，同时也保证知识并没有丢失，它们一直在笔记库里静静等待你的调用。</p>
<p>为什么不直接去互联网搜索？原因其实挺显而易见。互联网是变化的，一个信息来源，下一次去访问就不一定还存在，而且从搜索结果中剔除出需要的内容耗费的精力也不低，而这种精力本没必要每次都付出。</p>
<p>每个人都应该具有这样的一个系统：它能装下上文所述的那些零散知识，同时能够在你需要的时候方便地找到。我把这样的系统都叫做「个人 Wiki」。背后的实现方式重要性却没有那么高，你可以用印象笔记，可以用为知笔记，甚至纸笔、系统自带的备忘录也可以（其实系统自带备忘录挺强大的）。Wiki 这个词本来指的是多人维护的写作系统，但是我曲解它，把它解释成像维基百科那样由一个个词条组成的知识网络。</p>
<p>以我为例，我要求这个系统至少具有一项功能：搜索。能够搜索词条标题，不错；能够搜索词条全文，很好；能够搜索包含的附件内容，完美！从这个角度来看，印象笔记高级版完美实现了，它甚至还能搜索照片中出现的内容（应该是通过 OCR 实现的）。</p>
<p>当然，也可以选择自己搭建一个 Wiki 站点。方案有很多，最经济低成本的是基于 Hexo + GitHub Pages（或者国内的 Coding Pages）服务来搭建，这样不仅自己能有一个 Wiki 站点，由于内容发布到了互联网上，所以还有可能帮助到更多的人。</p>
<p>在选择站点主题时要注意一点，记得选择对分类和搜索功能支持良好的主题。有的主题虽然很美观，但是作为一个 Wiki 系统而言可能没有那么合适。我自己是选择了一个 Hexo 主题：<a href="https://github.com/zthxxx/hexo-theme-Wikitten">zthxxx/hexo-theme-Wikitten</a> 。这个主题的作者也说这是专门为个人 Wiki 设计的，支持多层级分类，支持全局搜索。我特别喜欢它的搜索功能，随着搜索词输入实时更新结果，非常舒服。</p>
<p>我的 Wiki：</p>
<p><a href="https://wiki.imalan.cn">熊猫小A的Wiki</a></p>
<p><strong>我真心呼吁每个人都搭建自己的一个 Wiki 系统，用什么方式实现，发布在哪里都不重要，但有和没有区别很大。</strong></p>
</div>
        
        <div class="prism-action-bar">
            <div class="comment-action action-item"><a class="no-link text-uppercase" href="/archives/108/#prism__comment" target="_self"><i class="fa fa-comment"></i>评论</a></div>
        </div>
        
    </article>
    
    <article class="yue prism-post-item col-md-8 offset-md-2">
        <h1 class="prism-post-title"><a class="no-link" href="/archives/107/" target="_self">一种使用 CMake 实现 C/C++ 与 CUDA 混合编译的方法</a></h1>
        <div class="prism-post-time">
            <a class="no-link" href="/archives/107/" target="_self">
                <time class="text-uppercase">
                    April 27 2018
                </time>
            </a>
        </div>
        <div class="prism-content"><p>学习 CUDA 的过程中，不免会需要将一部分 Host 代码和 CUDA 代码混合编译。比如在别的文件里面写好了要在 Device 上运行的 Kernel Code，需要在不同的 Host 代码里调用，同时给 Host 代码链接不同的库。在 Windows 上使用 Visual Studio 通过包含目录、设置链接器依赖项、为每个文件指定生成方式解决，这里介绍在 Ubuntu 上使用 CMake 生成 Makefile 的方法（我觉得在 Ubuntu 上开发效率会高一点，编译什么的都很快，而且不用跟复杂的 IDE 打交道）。</p>
<p>这个方法的原理是把 CUDA 代码编译为静态库，然后链接到 Host 代码上。以一个向量加法为例。</p>
<p>目录树如下：</p>

<pre><code>-- Source
    |--CMakeLists.txt
    |--host.cpp
    |--CudaCode
        |--CMakeLists.txt
        |--device.cuh
        |--device.cu</code></pre>
<p>host.cpp</p>

<pre><code>#include &lt;iostream&gt;
#include "CudaCode/device.cuh"

#define N 512

int main()
{
    int h_a[N];
    int h_b[N];
    int h_c[N];

    for (int i = 0; i &lt; N; ++i)
        h_a[i] = h_b[i] = i;

    h_vec_add(h_a, h_b, h_c, N);

    for (int i = 0; i &lt; N; ++i)
        std::cout &lt;&lt; h_c[i] &lt;&lt; '\t';
    std::cout &lt;&lt; '\n';
}</code></pre>
<p>CMakeLists.txt</p>

<pre><code># required cmake version
cmake_minimum_required(VERSION 2.8)

project(VecAdd)

add_executable(VecAdd host.cpp)

add_subdirectory(CudaCode)
target_link_libraries (VecAdd GPU)</code></pre>
<p>CudaCode/device.cuh</p>

<pre><code>#define BX 256

extern "C"
void h_vec_add(int *a,int *b,int *c,int n);</code></pre>
<p>CudaCode/device.cu</p>

<pre><code>#include &lt;device_launch_parameters.h&gt;
#include &lt;cuda_runtime.h&gt;
#include "device.cuh"

//Kernel
__global__ void d_vec_add(int *d_a, int *d_b, int *d_c,int n)
{
    int i = blockIdx.x * blockDim.x + threadIdx.x;
    if (i &lt; n)
        d_c[i] = d_a[i] + d_b[i];
}

extern "C" void h_vec_add(int *a, int *b, int *c, int n)
{
    int *d_a, *d_b, *d_c;
    cudaMalloc((void **)&amp;d_a, sizeof(int) * n);
    cudaMalloc((void **)&amp;d_b, sizeof(int) * n);
    cudaMalloc((void **)&amp;d_c, sizeof(int) * n);

    cudaMemcpy(d_a, a, sizeof(int) * n, cudaMemcpyHostToDevice);
    cudaMemcpy(d_b, b, sizeof(int) * n, cudaMemcpyHostToDevice);

    dim3 DimGrid(n / BX + 1, 1, 1);
    dim3 DimBlock(BX, 1, 1);

    d_vec_add&lt;&lt;&lt;DimGrid, DimBlock&gt;&gt;&gt;(d_a, d_b, d_c, n);

    cudaMemcpy(c, d_c, sizeof(int) * n, cudaMemcpyDeviceToHost);

    cudaFree(d_a);
    cudaFree(d_b);
    cudaFree(d_c);
}</code></pre>
<p>CudaCode/CMakeLists.txt</p>

<pre><code># required cmake version
cmake_minimum_required(VERSION 2.8)

project(GPU)

# packages
find_package(CUDA)

# nvcc flags
#set(CUDA_NVCC_FLAGS -O3;-G;-g)

file(GLOB_RECURSE CURRENT_HEADERS  *.h *.hpp *.cuh)
file(GLOB CURRENT_SOURCES  *.cpp *.cu)

source_group("Include" FILES ${CURRENT_HEADERS}) 
source_group("Source" FILES ${CURRENT_SOURCES}) 

cuda_add_library(GPU STATIC ${CURRENT_HEADERS} ${CURRENT_SOURCES})</code></pre>
<p>然后在顶层目录执行：</p>

<pre><code>mkdir BUILD
cd BUILD
ccmake ..</code></pre>
<p>按 <code>c</code>，出现 CMake 的配置界面，确定其中各项对应自己机器上的路径，不对的更改就好，然后按 <code>c</code> ，再按 <code>g</code> ，可见 BUILD 目录下已经生成了 Makefile，然后：输入 <code>make</code> ，显示编译完成。</p>
<p><figure class="pswp-item" style="flex: 82.18623481781377" data-width="812" data-height="494"><img src="https://cdn.jsdelivr.net/gh/AlanDecode/site-Blog@gh-pages/archives/assets/3cf86a0c8b573577ec8898e5f62d9d34.png" alt="httpsimg.imalan.cnimages20180427terminal-cmake" /><figcaption>httpsimg.imalan.cnimages20180427terminal-cmake</figcaption></figure></p>
<p>最后执行 <code>./VecAdd</code> 就能看到输出了。</p>
<p><figure class="pswp-item" style="flex: 82.96529968454259" data-width="1052" data-height="634"><img src="https://cdn.jsdelivr.net/gh/AlanDecode/site-Blog@gh-pages/archives/assets/186b5a4883aacb3d77fda2b9b0f99ae3.png" alt="httpsimg.imalan.cnimages20180427make-run" /><figcaption>httpsimg.imalan.cnimages20180427make-run</figcaption></figure></p>
<p>如果 host.cpp 还要链接别的什么库，在顶层的 CMakeLists.txt 中指定就好。</p>
<p>关于 CMake 的用法，参见 CMake 官方的一个例子，从头开始讲了 CMake 的一些基础用法，用来入门蛮好：<a href="https://cmake.org/cmake-tutorial/">CMake Tutorial | CMake</a> 。</p>
</div>
        
        <div class="prism-action-bar">
            <div class="comment-action action-item"><a class="no-link text-uppercase" href="/archives/107/#prism__comment" target="_self"><i class="fa fa-comment"></i>评论</a></div>
        </div>
        
    </article>
    
    <article class="yue prism-post-item col-md-8 offset-md-2">
        <h1 class="prism-post-title"><a class="no-link" href="/archives/105/" target="_self">CUDA 中的数组(Array)与纹理存储(Texture)介绍与使用</a></h1>
        <div class="prism-post-time">
            <a class="no-link" href="/archives/105/" target="_self">
                <time class="text-uppercase">
                    April 19 2018
                </time>
            </a>
        </div>
        <div class="prism-content"><p>CUDA 中有不同的内存类型，包括全局存储（Global Memory）、共享存储（Shared Memory）、常数存储器（Constant Memory）、寄存器（Register）等。这些不同类型的存储器在物理位置、访问效率上各有不同，因此适合不同的应用场景，这里不展开。这篇文章主要介绍一种存储器：纹理存储器（Texture Memory），以及如何使用它加速访问线性内存与 CUDA 数组（Array）。</p>
<p><em>本文大部分内容来自网络，我做了一些调整，加了一个例子。我经常解决了一个问题之后下次又碰到一样的问题，然后又是一翻搜寻。在那种时候我总是会想：“我当时有写文章就好了”，所以我觉得还是把平日里遇到的小问题、看到的好文章都整理整理总结总结，积少成多，对精进技术应该会有帮助。</em></p>
<h2>纹理存储器（Texture Memory）</h2>
<h3>纹理存储器是什么</h3>
<p>首先明确：纹理存储器是一种只读存储器，由 GPU 用于纹理渲染的图形专用单元发展而来。它同样位于显存中，但是在访问时有许多优秀的特性，例如可通过纹理缓存加速读取。相对同样具有缓存功能的常数存储器（Constant Memory），纹理存储器可以绑定更大的数据，并且支持一维、二维、三维纹理，并可以通过浮点数寻址。由于纹理存储器转为图像纹理渲染而设计，它特别适合图像处理、查找表等，对随机访问与非对齐访问有良好的加速效果，并且可以按需在返回时同时进行滤波等操作。一个很直观的例子，下面的矩阵中 1,2,3,4 四个数据在行优先的线性存储中并不具有相邻的物理地址，因此连续索引这些数据效率是低下的；但是纹理存储可以在读取其中某个数据时将临近的值载入缓存（Cache），这样下次访问时，则可以直接命中缓存，减少对 Global Memory 的访问，从而提高效率。</p>
<p><figure class="pswp-item" style="flex: 50.27075812274368" data-width="557" data-height="554"><img src="https://cdn.jsdelivr.net/gh/AlanDecode/site-Blog@gh-pages/archives/assets/482033fc1e4efd3fbcc0e94674d2f969.png" alt="textures_cuda" /><figcaption>textures_cuda</figcaption></figure></p>
<p>对程序员来说，纹理存储是透明的，程序员无需关心内部实现。</p>
<p>在 CUDA 内核函数（Kernel Code）中访问纹理存储器称作纹理拾取（Texture Fetching），作为纹理存储的特性，纹理拾取与普通的访问某一显存地址的数据有很大不同。纹理拾取时采用的坐标可以不同于数据在存储中的真实地址，二者间的转换通过纹理参照系（Texture Reference）来约定，将显存中的数据与约定的纹理参照系关联的操作称为纹理绑定（Texture Binding）。可以与纹理绑定的数据有两种：显存中的线性内存（Linear Memory）和CUDA数组（Array）。线性内存只能与一维纹理绑定，CUDA 数组则可绑定一维、二维、三维纹理。</p>
<p>关于纹理缓存，它有两个作用。首先纹理缓存中的数据可被复用，当访问的数据已经位于缓存中时，访问该数据将命中缓存，减少对 Global Memory 的访问；其次纹理存储可以将拾取坐标附近的几个像元缓存起来，并可以在拾取时进行插值。</p>
<p>关于纹理缓存需要注意的点：由于纹理缓存是只读的，如果你更改了绑定到纹理缓存的数据，纹理缓存中的数据可能并没有被更新，此时拾取到的可能是错误的数据。因此每次修改原始数据都要重新绑定。对绑定 CUDA 数组时不存在此问题（Device 端 CUDA 数组是只读的），但绑定线性内存则要特别注意。</p>
<h3>纹理拾取（Texture Fetching）</h3>
<p>访问纹理中的数据的过程称作纹理拾取。对绑定线性内存的纹理，拾取纹理的坐标只能是定点型、坐标与内存的真实偏移量（Offset）相同。对绑定 CUDA 数组的纹理，拾取坐标是浮点型，并支持许多特殊功能。</p>
<p><strong>浮点形式寻址：</strong>访问时采用浮点型的坐标对纹理进行寻址，也就是说坐标无需是整数。寻址的方式可以使归一化或者非归一化的。对一个坐标范围是 [0:N]*[0:N] 的纹理，当使用归一化的寻址方式时，每个维度上的坐标被映射到 [0.0:1.0f) 的范围中；使用非归一化寻址时，将被映射到 [0.0:N.0f) 的范围内。当访问的坐标不在一个像元的中心时，根据选择的滤波模式不同，将返回不同的值。</p>
<p><strong>滤波模式：</strong>仅对绑定 CUDA 数组的纹理有效。当使用浮点型的坐标寻址纹理时，将根据设定返回不同类型的值。设定可以有：最邻近取样模式（<strong>这里存疑，因为我自己试了一下似乎也不是最近邻，而是直接对坐标取 floor</strong>）和线性滤波模式，这无需多说。需要注意的是，当开启了线性滤波模式时，要对访问的坐标多加注意，注意不要因为线性插值的原因得到错误的值，例如当要访问原本是 (i,j) 的坐标时可能需要转为 (i+0.5f,j+0.5f)，我在 StackOverflow 上看到对这个特性的讲解（自己翻译了一下）：</p>
<blockquote><p>在图形学中，纹理指的是用于描述一个平面的采样点集。也就是说，纹理中的一个点指的就是这个平面在此处的采样，并不存在大小，这就与像素（Pixel）不同，一个 Pixel 是具有空间大小的。</p>
<p>因此 +0.5f 操作保证寻址的位置正好是真正的整数点对应的位置。</p>
</blockquote>
<p>对于最近邻插值得返回值当然无此限制。</p>
<p><strong>寻址模式：</strong>仅对绑定 CUDA 数组的纹理有效，规定了当寻址的坐标超出允许的寻址范围时的行为。有钳位模式与循环模式两种。钳位模式下，当寻址坐标超出了边界，则钳位到最近边界；循环模式下，则是做求模处理。例如纹理坐标范围 [0,1)，访问 1.25 时，钳位模式返回 0.999……处的值，循环模式返回 0.25 处的值。</p>
<p><strong>类型转换：</strong>当像元中数据是 8 bit 或者 16 bit 时，可对拾取的值进行类型转换，映射到 [0.0f,1.0f] 或者 [-1.0f,1.0f]。</p>
<hr>
<p><strong>若要使用纹理内存，大致分为这么几步：首先在 Host 端声明要绑定的线性内存或者 CUDA 数组；其次设置好纹理参照系；然后将纹理参照系与线性内存或者 CUDA 数组绑定；最后在 Kernel 代码中访问纹理内存即可。</strong></p>
<hr>
<h2>CUDA 数组（Array）</h2>
<p>显存中可以分配的空间有两种：线性内存和 CUDA 数组，都可以与纹理参照系绑定，但是 CUDA 数组对纹理拾取有优化，并且在设备端只能通过纹理拾取访问。</p>
<h3>cudaChannelFormatDesc</h3>
<p>在声明一个 CUDA 数组前，首先要以结构体 <code>cudaChannelFormatDesc</code> 设置 CUDA 数组的数据类型。</p>
<div class="highlight"><pre><span></span><span class="k">struct</span> <span class="n">__device_builtin__</span> <span class="n">cudaChannelFormatDesc</span>
<span class="p">{</span>
    <span class="kt">int</span>                        <span class="n">x</span><span class="p">;</span> <span class="cm">/**&lt; x */</span>
    <span class="kt">int</span>                        <span class="n">y</span><span class="p">;</span> <span class="cm">/**&lt; y */</span>
    <span class="kt">int</span>                        <span class="n">z</span><span class="p">;</span> <span class="cm">/**&lt; z */</span>
    <span class="kt">int</span>                        <span class="n">w</span><span class="p">;</span> <span class="cm">/**&lt; w */</span>
    <span class="k">enum</span> <span class="n">cudaChannelFormatKind</span> <span class="n">f</span><span class="p">;</span> <span class="cm">/**&lt; Channel format kind */</span>
<span class="p">};</span>
</pre></div>
<p>其中，<code>x,y,z,w</code> 表示返回每个成员的位数，<code>cudaChannelFormatKind</code>表示成员类型，这是一个枚举类型，可以取值：</p>
<p><code>cudaChannelFormatKindSigned</code>：有符号整型</p>
<p><code>cudaChannelFormatKindUnsigned</code>：无符号整型</p>
<p><code>cudaChannelFormatKindFloat</code>：浮点型</p>
<p><code>cudaChannelFormatKindNone</code>：无类型</p>
<p><code>cudaChannelFormatDesc</code> 可以用一个简单的函数构造：<code>struct cudaChannelFormatDesc cudaCreateChannelDesc(int x, int y, int z, int w, enum cudaChannelFormatKind f);</code>。在只需要类型信息时，可以使用简化版的：<code>struct cudaChannelFormatDesc cudaCreateChannelDesc&lt;T&gt;();</code>。</p>
<h3>cudaExtent</h3>
<p>然后需要确定 CUDA 数组的纬度与尺寸。纬度与尺寸通过一个结构体 <code>cudaExtent</code> 描述：</p>
<div class="highlight"><pre><span></span><span class="k">struct</span> <span class="n">__device_builtin__</span> <span class="n">cudaExtent</span>
<span class="p">{</span>
    <span class="kt">size_t</span> <span class="n">width</span><span class="p">;</span>     <span class="cm">/**&lt; Width in elements when referring to array memory, in bytes when referring to linear memory */</span>
    <span class="kt">size_t</span> <span class="n">height</span><span class="p">;</span>    <span class="cm">/**&lt; Height in elements */</span>
    <span class="kt">size_t</span> <span class="n">depth</span><span class="p">;</span>     <span class="cm">/**&lt; Depth in elements */</span>
<span class="p">};</span>
</pre></div>
<p>成员分别代表长宽高。可以用一个函数来构造：<code>struct cudaExtent make_cudaExtent(size_t w, size_t h, size_t d)</code>。</p>
<h3>为 CUDA 数组分配空间</h3>
<p>CUDA 数组可通过 <code>cudaMalloc3DArray()</code> 或者 <code>cudaMallocArray()</code> 分配空间。其中， <code>cudaMalloc3DArray()</code> 可分配一维、二维、三维空间，而 <code>cudaMallocArray()</code> 一般用于分配二维数组。使用完数组后，需要使用 <code>cudaFreeArray()</code> 释放空间。</p>
<h3>拷贝数据到 CUDA 数组</h3>
<p>对普通线性内存，使用 <code>cudaMemcpy()</code>，对 CUDA 数组使用 <code>cudaMemcpy2D()</code>、<code>cudaMemcpy3D()</code>。注意这里可能比较 tricky，根据显卡的性能和资源限制，拷贝的大小不能过大。</p>
<h2>纹理参考系（Texture Reference）</h2>
<h3>纹理参考系声明</h3>
<p>纹理参考系中的某些属性需要在编译期确定。texture 类型继承自 textureReference，类似这样构造：<code>texture&lt;T, texType, cudaTextureReadMode&gt; tex;</code>。</p>
<p><code>T</code> 指明由纹理拾取返回的数据类型。可以是基本整形，或者单精度浮点型组成的 1-，2-，4- 元组向量类型。</p>
<p><code>texType</code> 指明了纹理排布方式。取值：<code>cudaTextureType1D</code>、<code>cudaTextureType3D</code>、<code>cudaTextureType3D</code>、<code>cudaTextureType1DLayered</code>、<code>cudaTextureType2DLayered</code>等。</p>
<p><code>cudaTextureReadMode</code> 可取值 <code>cudaReadModeNormalizedFloat</code> 或者 <code>cudaReadModeElementType</code>，本参数可选，缺省 <code>cudaReadModeElementType</code>。当取 <code>cudaReadModeNormalizedFloat</code> 时，若 T 为整型，视 T 是否有符号则将被映射到 [-1.0f,1.0f]或者 [0.0f,1.0f]。 <code>cudaReadModeElementType</code>不对输出转换。</p>
<h3>设置运行时的纹理参考系属性</h3>
<p>除以上需要在编译器确定的属性外，纹理参考系有可在运行时确定的属性：</p>
<div class="highlight"><pre><span></span><span class="k">struct</span> <span class="n">__device_builtin__</span> <span class="n">textureReference</span>
<span class="p">{</span>
    <span class="kt">int</span>                          <span class="n">normalized</span><span class="p">;</span>
    <span class="k">enum</span> <span class="n">cudaTextureFilterMode</span>   <span class="n">filterMode</span><span class="p">;</span>
    <span class="k">enum</span> <span class="n">cudaTextureAddressMode</span>  <span class="n">addressMode</span><span class="p">[</span><span class="mi">3</span><span class="p">];</span>
    <span class="k">struct</span> <span class="n">cudaChannelFormatDesc</span> <span class="n">channelDesc</span><span class="p">;</span>
<span class="p">};</span>
</pre></div>
<p><code>normalized</code>：返回是否归一化，如上文所述。对一个坐标范围是 [0:N]*[0:N] 的纹理，当使用归一化的寻址方式时，每个维度上的坐标被映射到 [0.0:1.0f) 的范围中；使用非归一化寻址时，将被映射到 [0.0:N.0f) 的范围内。</p>
<p><code>filterMode</code>：滤波模式。仅对绑定 CUDA 数组的纹理有效。当使用浮点型的坐标寻址纹理时，将根据设定返回不同类型的值。设定可以有：<code>cudaFilterModePoint</code>和<code>cudaFilterModeLinear</code>。分别表示最近邻插值（<strong>如上文所述，存疑。我觉得好像是直接对浮点坐标做了 floor</strong>）和线性插值。</p>
<p><code>addressMode[3]</code>：寻址模式，即如何处理越界的纹理坐标。可设置：<code>cudaAddressModeClamp</code>和<code>cudaAddressModeWrap</code>。Clamp 即钳位模式，Wrap 为循环模式。循环模式只支持归一化的纹理坐标。</p>
<p><code>channelDesc</code>：描述纹理返回值类型，同 CUDA 数组部分的内容。</p>
<h3>纹理绑定</h3>
<p>使用 <code>cudaBindTextureToArray();</code> 或者 <code>cudaBindTexture();</code> 或者 <code>cudaBindTexture2D();</code>。</p>
<p>取消绑定：<code>cudaUnbindTexture()</code>。</p>
<h3>纹理拾取</h3>
<p>根据不同的纹理类型，采用不同的方式来拾取。与线性内存绑定的纹理，使用<code>texfetch1D()</code>来拾取；对 CUDA 数组，使用 <code>tex1D()</code>、<code>tex2D()</code>、<code>tex3D()</code>来拾取，并使用浮点坐标。</p>
<h2>一个例子</h2>
<p>举个例子，当前有一个 <code>float matrix[3][4]</code> ，将其载入显存，分配为 CUDA 2D 数组，作为纹理使用，并在 kernel code 中拾取。假设当前 matrix 已经初始化完毕。</p>
<h3>代码</h3>
<p>纹理声明：</p>
<div class="highlight"><pre><span></span><span class="n">texture</span><span class="o">&lt;</span><span class="kt">float</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="n">cudaReadModeElementType</span><span class="o">&gt;</span> <span class="n">tex_mat</span><span class="p">;</span>
</pre></div>
<p>Host Code：</p>
<div class="highlight"><pre><span></span><span class="kt">int</span> <span class="nf">main</span><span class="p">()</span>
<span class="p">{</span>
    <span class="p">...</span>

    <span class="n">cudaError</span> <span class="n">err</span><span class="p">;</span>
    <span class="n">cudaArray</span> <span class="o">*</span><span class="n">arr_mat</span><span class="p">;</span>
    <span class="n">cudaChannelFormatDesc</span> <span class="n">channelDesc</span> <span class="o">=</span> <span class="n">cudaCreateChannelDesc</span><span class="o">&lt;</span><span class="kt">float</span><span class="o">&gt;</span><span class="p">();</span>
    <span class="n">cudaMallocArray</span><span class="p">((</span><span class="n">cudaArray</span><span class="o">**</span><span class="p">)</span><span class="o">&amp;</span><span class="n">arr_mat</span><span class="p">,</span><span class="o">&amp;</span><span class="n">channelDesc</span><span class="p">,</span><span class="mi">4</span><span class="p">,</span><span class="mi">3</span><span class="p">);</span>
    <span class="n">err</span> <span class="o">=</span> <span class="n">cudaMemcpyToArray</span><span class="p">(</span><span class="n">arr_mat</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">matrix</span><span class="p">,</span> <span class="mi">3</span> <span class="o">*</span> <span class="mi">4</span><span class="o">*</span><span class="k">sizeof</span><span class="p">(</span><span class="kt">float</span><span class="p">),</span> <span class="n">cudaMemcpyHostToDevice</span><span class="p">);</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">cudaSuccess</span> <span class="o">!=</span> <span class="n">err</span><span class="p">)</span>
        <span class="n">printf</span><span class="p">(</span><span class="s">&quot;CUDA ERROR : %s </span><span class="se">\n</span><span class="s">&quot;</span><span class="p">,</span> <span class="n">cudaGetErrorString</span><span class="p">(</span><span class="n">err</span><span class="p">));</span>

    <span class="n">tex_mat</span><span class="p">.</span><span class="n">normalized</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
    <span class="n">tex_mat</span><span class="p">.</span><span class="n">filterMode</span> <span class="o">=</span> <span class="n">cudaFilterModePoint</span><span class="p">;</span>
    <span class="c1">//tex_mat.filterMode = cudaFilterModeLinear;</span>
    <span class="n">tex_mat</span><span class="p">.</span><span class="n">addressMode</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">cudaAddressModeClamp</span><span class="p">;</span>
    <span class="n">tex_mat</span><span class="p">.</span><span class="n">addressMode</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">cudaAddressModeClamp</span><span class="p">;</span>
    <span class="n">tex_mat</span><span class="p">.</span><span class="n">addressMode</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="n">cudaAddressModeClamp</span><span class="p">;</span>
    <span class="n">tex_mat</span><span class="p">.</span><span class="n">channelDesc</span> <span class="o">=</span> <span class="n">channelDesc</span><span class="p">;</span>

    <span class="n">err</span> <span class="o">=</span> <span class="n">cudaBindTextureToArray</span><span class="p">(</span><span class="n">tex_mat</span><span class="p">,</span> <span class="p">(</span><span class="n">cudaArray</span><span class="o">*</span><span class="p">)</span><span class="n">arr_mat</span><span class="p">,</span><span class="n">channelDesc</span><span class="p">);</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">cudaSuccess</span> <span class="o">!=</span> <span class="n">err</span><span class="p">)</span>
        <span class="n">printf</span><span class="p">(</span><span class="s">&quot;CUDA ERROR : %s </span><span class="se">\n</span><span class="s">&quot;</span><span class="p">,</span> <span class="n">cudaGetErrorString</span><span class="p">(</span><span class="n">err</span><span class="p">));</span>

    <span class="n">dim3</span> <span class="n">dimBlock</span> <span class="o">=</span> <span class="n">dim3</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">);</span>
    <span class="n">dim3</span> <span class="n">dimGrid</span> <span class="o">=</span> <span class="n">dim3</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">);</span>

    <span class="n">texture_fetching</span> <span class="o">&lt;&lt;&lt;</span><span class="n">dimGrid</span><span class="p">,</span> <span class="n">dimBlock</span> <span class="o">&gt;&gt;&gt;</span> <span class="p">();</span>

    <span class="n">err</span> <span class="o">=</span> <span class="n">cudaUnbindTexture</span><span class="p">(</span><span class="n">tex_mat</span><span class="p">);</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">cudaSuccess</span> <span class="o">!=</span> <span class="n">err</span><span class="p">)</span>
        <span class="n">printf</span><span class="p">(</span><span class="s">&quot;CUDA ERROR : %s </span><span class="se">\n</span><span class="s">&quot;</span><span class="p">,</span> <span class="n">cudaGetErrorString</span><span class="p">(</span><span class="n">err</span><span class="p">));</span>

    <span class="n">err</span> <span class="o">=</span> <span class="n">cudaFreeArray</span><span class="p">(</span><span class="n">arr_mat</span><span class="p">);</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">cudaSuccess</span> <span class="o">!=</span> <span class="n">err</span><span class="p">)</span>
        <span class="n">printf</span><span class="p">(</span><span class="s">&quot;CUDA ERROR : %s </span><span class="se">\n</span><span class="s">&quot;</span><span class="p">,</span> <span class="n">cudaGetErrorString</span><span class="p">(</span><span class="n">err</span><span class="p">));</span>

    <span class="p">...</span>
<span class="p">}</span>
</pre></div>
<p>Kernel Code：</p>
<div class="highlight"><pre><span></span><span class="n">__global__</span>
<span class="kt">void</span> <span class="nf">texture_fetching</span><span class="p">()</span>
<span class="p">{</span>
    <span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">y</span><span class="p">;</span>
    <span class="kt">int</span> <span class="n">j</span> <span class="o">=</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">;</span>

    <span class="k">if</span> <span class="p">(</span><span class="n">i</span> <span class="o">==</span> <span class="mi">0</span> <span class="o">&amp;&amp;</span> <span class="n">j</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span>
    <span class="p">{</span>
        <span class="n">printf</span><span class="p">(</span><span class="s">&quot;Texture Fetch:</span><span class="se">\n</span><span class="s">&quot;</span><span class="p">);</span>
        <span class="k">for</span> <span class="p">(</span><span class="kt">float</span> <span class="n">idxy</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">idxy</span> <span class="o">&lt;</span> <span class="mi">3</span><span class="p">;</span> <span class="n">idxy</span><span class="o">++</span><span class="p">)</span>
        <span class="p">{</span>
            <span class="k">for</span> <span class="p">(</span><span class="kt">float</span> <span class="n">idxx</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">idxx</span> <span class="o">&lt;</span> <span class="mi">4</span><span class="p">;</span> <span class="n">idxx</span><span class="o">++</span><span class="p">)</span>
            <span class="p">{</span>
                <span class="n">printf</span><span class="p">(</span><span class="s">&quot;%f &quot;</span><span class="p">,</span> <span class="n">tex2D</span><span class="p">(</span><span class="n">tex_mat</span><span class="p">,</span> <span class="n">idxx</span><span class="p">,</span> <span class="n">idxy</span><span class="p">));</span>
            <span class="p">}</span>
            <span class="n">printf</span><span class="p">(</span><span class="s">&quot;</span><span class="se">\n</span><span class="s">&quot;</span><span class="p">);</span>
        <span class="p">}</span>
        <span class="n">printf</span><span class="p">(</span><span class="s">&quot;</span><span class="se">\n</span><span class="s">Some other points: </span><span class="se">\n</span><span class="s">&quot;</span><span class="p">);</span>        
        <span class="n">printf</span><span class="p">(</span><span class="s">&quot;tex_mat(0.1,1.0) = %f</span><span class="se">\n</span><span class="s">&quot;</span><span class="p">,</span> <span class="n">tex2D</span><span class="p">(</span><span class="n">tex_mat</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mf">1.</span><span class="p">));</span>
        <span class="n">printf</span><span class="p">(</span><span class="s">&quot;tex_mat(1.4,2.0) = %f</span><span class="se">\n</span><span class="s">&quot;</span><span class="p">,</span> <span class="n">tex2D</span><span class="p">(</span><span class="n">tex_mat</span><span class="p">,</span> <span class="mf">1.4</span><span class="p">,</span> <span class="mf">2.</span><span class="p">));</span>
        <span class="n">printf</span><span class="p">(</span><span class="s">&quot;tex_mat(0.999,2.0) = %f</span><span class="se">\n</span><span class="s">&quot;</span><span class="p">,</span> <span class="n">tex2D</span><span class="p">(</span><span class="n">tex_mat</span><span class="p">,</span> <span class="mf">0.999</span><span class="p">,</span> <span class="mf">2.</span><span class="p">));</span>
        <span class="n">printf</span><span class="p">(</span><span class="s">&quot;tex_mat(1.0,3.5) = %f</span><span class="se">\n</span><span class="s">&quot;</span><span class="p">,</span> <span class="n">tex2D</span><span class="p">(</span><span class="n">tex_mat</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">3.5</span><span class="p">));</span>
    <span class="p">}</span>
<span class="p">}</span>
</pre></div>
<h3>结果</h3>
<p>当 <code>tex_mat.filterMode = cudaFilterModePoint;</code> 时输出：</p>
<p><figure class="pswp-item" style="flex: 78.75751503006012" data-width="786" data-height="499"><img src="https://cdn.jsdelivr.net/gh/AlanDecode/site-Blog@gh-pages/archives/assets/fb9a09bc92ec21f669024318c3b6779b.png" alt="filtpoint" /><figcaption>filtpoint</figcaption></figure></p>
<p>当 <code>tex_mat.filterMode = cudaFilterModeLinear;</code> 时输出：</p>
<p><figure class="pswp-item" style="flex: 71.93158953722335" data-width="715" data-height="497"><img src="https://cdn.jsdelivr.net/gh/AlanDecode/site-Blog@gh-pages/archives/assets/b9df65c6b89a806ea5cfc821f99b153a.png" alt="filtlinear" /><figcaption>filtlinear</figcaption></figure></p>
</div>
        
        <div class="prism-action-bar">
            <div class="comment-action action-item"><a class="no-link text-uppercase" href="/archives/105/#prism__comment" target="_self"><i class="fa fa-comment"></i>评论</a></div>
        </div>
        
    </article>
    
    <article class="yue prism-post-item col-md-8 offset-md-2">
        <h1 class="prism-post-title"><a class="no-link" href="/archives/103/" target="_self">《千年女优》：像我这样寻找的人</a></h1>
        <div class="prism-post-time">
            <a class="no-link" href="/archives/103/" target="_self">
                <time class="text-uppercase">
                    March 31 2018
                </time>
            </a>
        </div>
        <div class="prism-content"><p>《千年女优》整部电影里面真实与幻境交替，主角作为寻常女子和作为所扮演的角色身份混杂在一起，从不同角度不同时间不同空间来刻画女主一生的追寻。电影的实验性质很强烈，从配乐上就能感受出来：这无论怎样都不是当谈起“刻画二战时一位女演员的爱情故事的电影”时我会构想出的音乐。</p>
<div class="Meting">
[Music server="netease" id="29774180" type="song"/]</div><p>千代子不是在追寻她爱的那个人，她只是在追寻「追寻」本身，而追寻就是爱情，她爱上的是爱情本身而已。「我喜欢追寻着你的我自己」，这一点不仅在电影的最后点明了，电影中也有暗示。「追寻」是一种仪式，通过追寻，千代子显然完成了对自己的补完。</p>
<p>这是电影 60 分钟左右开始时长 6 分钟左右的片段。如果这也算「日剧跑」的话，那就是我见过最美的日剧跑：</p>
<div class="dplayer"  data-url="https://static.imalan.cn/video/MillenniumActressh264.mp4"></div></div>
        
        <div class="prism-action-bar">
            <div class="comment-action action-item"><a class="no-link text-uppercase" href="/archives/103/#prism__comment" target="_self"><i class="fa fa-comment"></i>评论</a></div>
        </div>
        
    </article>
    
    <article class="yue prism-post-item col-md-8 offset-md-2">
        <h1 class="prism-post-title"><a class="no-link" href="/archives/102/" target="_self">春天=过敏，以及用 Apple Script 调整 Mac 屏幕亮度</a></h1>
        <div class="prism-post-time">
            <a class="no-link" href="/archives/102/" target="_self">
                <time class="text-uppercase">
                    March 30 2018
                </time>
            </a>
        </div>
        <div class="prism-content"><p>都说北京四季不分明，每年都在夏天和冬天之间轮替，我说这是瞎扯。每年只要发现自己眼睛痒、鼻子堵、打喷嚏，我就知道不是春天就是秋天又到了。这几天被过敏折腾得不行，随时保持眼泪汪汪的状态，卫生纸消耗速度惊人，赶紧去医院看病开了些抗过敏的药才好转一点。</p>
<h3>又来折腾 Hackintosh</h3>
<p>去年末把笔记本从惠普 Probook 440 G3 换成了联想 R720，于是之前一直在用的黑苹果也木有了，新电脑没抽出时间来折腾它。前两天临时起意，就随便搜索了一下这个机型有没有成功的例子，找到了一个类似的：<a href="http://tieba.baidu.com/p/5521219636?pv=1&amp;red_tag=a1580648294">【教程】联想拯救者R720 安装MacOS 10.12.6 双系统教程</a> 。虽然帖子里的机型是 i5-7300 的版本，我的是 i7-7700HQ 的版本，据我估计差别不会太大，而且楼主封装的镜像和引导以及驱动<strong>非常全面</strong> ，弄得我顿时手痒起来。回头一看自己的固态，500G 就用了 200 多 G，闲着也是闲着，分了 100G 出来装 Hackintosh。</p>
<p>版本是 10.12.6，主要是一般来说，上一版本的驱动会更稳定一些，而且可以找的资料也多一点。过程很顺利，果然联想爆款一向装黑苹果都不难，回头写个过程记录当备忘。总之结果是：</p>
<ul>
<li>核显 Intel HD Graphics 630，独立显卡无解。亮度调节 OK</li>
<li>有线网卡 OK，无线无解</li>
<li>声卡 OK（扬声器以及耳机）</li>
<li>电池状态显示 OK</li>
<li>蓝牙 OK，但没有 AirDrop 和 Handoff</li>
<li>iMessage、iCloud、App Store 这些内置程序都正常。</li>
<li>睡眠一睡就死</li>
<li>触控板依然很垃圾（当然这是硬件的锅）</li>
</ul>
<p>算是比较标准的黑苹果状态。就差无线网卡比较蛋疼，家里有一块 BCM94352Z，但不在手边，回头把无线网卡换一个试试能不能把 WiFi 修好。其实我装黑苹果也不是要做什么 iOS 开发啥的，就是很喜欢 macOS 的那种顺畅和人性化的感觉，外观设计和字体渲染也很棒，最最重要的还是 Mac 平台的一些很精致的独占应用。既然没钱买一台真 Mac，暂时就这么曲线救国吧~</p>
<h3>用 Apple Script 调整 Mac 屏幕亮度</h3>
<p>一个很烦人的点，跟黑苹果无关，就是 Mac 外接显示器时是没办法（方便地）在不合盖的情况下关闭内建显示器的。我平时主要是用外接显示器的，很少的情况下才会镜像显示器或者扩展显示器。这个问题可以用两种方法解决。</p>
<h4>糊弄电脑</h4>
<p>说白了就是让电脑觉得已经盒盖了。方法就是拿一块小磁铁在笔记本 C 面试探，找到一个位置，当磁铁在这个位置时笔记本感应器接收到信号，觉得盖子合上了，于是关闭内建显示器。对我的联想 R720 而言，这个位置在左侧耳机孔附近。</p>
<p>这个方法的好处是可以真正关闭内建显示器；坏处就是必须有个磁铁一直放在那个位置。</p>
<h4>糊弄自己</h4>
<p>原理就是把亮度开到最低，骗自己已经关闭了显示器……</p>
<p>虽然有点自欺欺人，但是并不是不能接受的做法。不过每次都要用功能键（何况 Hackintosh 很多时候功能键并不奏效）或者去偏好设置里调节多少有些不便，我找了个用 Apple Script 的方法。</p>
<p>首先打开系统内置的脚本编辑器，新建文稿，输入以下代码：</p>

<pre><code>tell application "System Preferences"
    activate
    reveal anchor "displaysDisplayTab" of pane id "com.apple.preference.displays"
    tell application "System Events"
        set now to value of slider 1 of group 1 of tab group 1 of window "内建显示器" of process "System Preferences"
        if now &gt; 0 then
            set value of slider 1 of group 1 of tab group 1 of window "内建显示器" of process "System Preferences" to 0
        else if now = 0 then
            set value of slider 1 of group 1 of tab group 1 of window "内建显示器" of process "System Preferences" to 1
        end if
    end tell
    quit
end tell</code></pre>
<p>然后点击文件-导出，文件格式选择应用程序，勾选仅运行，然后起一个好听的名字，比如“嗯嗯这样的话我就可以随便开关屏幕了呢.app”，然后保存。打开系统偏好设置-安全性与隐私，隐私选项卡里，在“辅助功能”这一项里点“+”，添加上面保存的“嗯嗯这样的话我就可以随便开关屏幕了呢.app”。</p>
<p>以后点击“嗯嗯这样的话我就可以随便开关屏幕了呢.app”就会把内建显示器亮度调成 0，再点就会调成 1，再点就调成 0……</p>
<div class="scode">注意：以上代码适用于中文系统，如果不是中文系统，那要把代码里的“内建显示器”换成对应的窗口名，比如（我猜）"Built-in Display" 啥的。</div><p>如果借用 Alfred 这类可以运行 Apple Script 的软件，那么可以更加方便，设置触发短语直接运行上面的代码即可。置的亮度值可以自己设定，不一定非得是 0 和 1。这段代码可以给我们许多启示：代表着我们还能用 Apple Script 调整更多的系统偏好设置，exciting！</p>
</div>
        
        <div class="prism-action-bar">
            <div class="comment-action action-item"><a class="no-link text-uppercase" href="/archives/102/#prism__comment" target="_self"><i class="fa fa-comment"></i>评论</a></div>
        </div>
        
    </article>
    
    <article class="yue prism-post-item col-md-8 offset-md-2">
        <h1 class="prism-post-title"><a class="no-link" href="/archives/101/" target="_self">关于最近的一点 Update【2018-03】</a></h1>
        <div class="prism-post-time">
            <a class="no-link" href="/archives/101/" target="_self">
                <time class="text-uppercase">
                    March 26 2018
                </time>
            </a>
        </div>
        <div class="prism-content"><p>开学也快一个月了，毕业设计什么的在有条不紊（毫无头绪）地进行，不过最近身上的任务还不算重，有时间折腾点硬件软件。简单记点流水账吧。</p>
<h2>Surface Pro</h2>
<p>前段时间发神经一样地出掉了手上的 iPad Pro 9.7 寸，出发点主要有几个：iPad Pro 说实话还是比较难参与到我的日常学习工作中来，码代码的话，iPad Pro 写点脚本语言，Python、JS 什么的还勉勉强强，用来写 C、C++ 实在是不适合，另外 iPad Pro 确实没有便携、舒适的外接键盘，这是真话，即使是 Smart Keyboard 也缺点火候。我用 iPad Pro 主要的用处基本上就是看 PDF，在这一点上，配上 Apple Pencil 的 iPad Pro 确实是无敌的。</p>
<p>总之我还是咬咬牙入手了 Surface Pro 乞丐版，体验了一周左右，现在怎么说呢……中规中矩吧，也没有特别惊艳的感觉。</p>
<p>我有一个失误：以为配上了 Surface Pen 的 Surface Pro 可以至少与 iPad Pro 一拼，其实在触控、书写体验上，真的还差得远。差到我赶紧把 Surface Pen 给退了，因为我知道我基本上不会去用它。之所以说是失误，现在的 Surface Pro 基本上被我当做一个普通笔记本来用了，白白浪费了触控功能。其实这个价格我完全可以去买一台小米笔记本的……</p>
<p>好的地方嘛，作为一台平板来讲，有<strong>出色</strong>的键盘和触控板（除了 Surface 系列，我基本没看到过不是垃圾的 Windows 笔记本触控板，），美妙的全功能 Win10，比较优秀的屏幕，还有让我很满意的 Windows Hello；作为 Windows 笔记本来讲，轻薄便携，续航优秀，外形酷炫，非常帅气。</p>
<p>从使用的角度来讲，最轻度的笔记本电脑使用需求：网页浏览，Typora 写点小文章，Office 套件，一些基础的 Win32 应用、UWP 应用都毫无问题；中度使用，VS Code 写点 Python 啥的也 OK；重度一点的嘛……我是勉勉强强把 VS 2015 和 Matlab 给装上了，只能说，非常非常勉强，我是没有用来干活的欲望，顶多借助 VS 的智能跳转和智能提示看看我用 Dropbox 同步过来的项目，真正干货还是滚回我那个又大又厚又重又丑陋的联想 R720。</p>
<p>现在的期待：<strong>求大佬们赶紧搞好 New Surface Pro 的 Ubuntu 驱动！我要装 Ubuntu！触屏什么的无所谓，只要能连 WiFi 和蓝牙以及键盘能用我就满足了！</strong></p>
<h2>WSL</h2>
<p>Windows Subsystem for Linux，简称 WSL，让我们能在 Windows 里直接使用 Linux。关于这个的教程蛮多的。</p>
<p>我主要使用的系统是 Windows，但是有时候要进行一些开发调试，比如这个博客，本地有个 LNMP 环境还是比在远程服务器上搞轻松一点的。开始我是用的虚拟机装 Ubuntu，资源消耗有点厉害，后来听说了 WSL 就想试着看看能不能部署好。</p>
<p>但是失败了……不管是 PHP 还是 Nginx 还是 MySQL 在 WSL 里的表现都怪怪的，学艺不精，搞不明白，所以暂时就放那儿了。</p>
<p>不过我也没把 WSL 卸了，有个 Bash 环境很方便，某些小的命令行工具用起来很舒服的~</p>
<h2>环太平洋</h2>
<p>今晚刚去看，我是第一部的粉丝，真心没有办法拒绝这种机甲的设定！</p>
<p>之所以喜欢机甲，其实很大程度上是因为 EVA。所以虽然这第二部评价不佳，但是我看到了更多借鉴 EVA 的元素，弄得我很兴奋~</p>
<p>比如一开始，可以说是把 EVA 中的量产机、无人驾驶机、dummy system 几个设定揉在了一起，甚至还有侵入性使徒这种设定的影子，后来东京那场戏让我觉得可能把第三新东京市都造好了……行人躲进一个电梯一样的东西里，这玩意儿就径直下地了，这不是第三新东京市是啥哈哈。路边的铁桩、门上的警戒标识基本上完全是 EVA 的风格，甚至连机甲本身都更像 EVA 了：有一架（名字忘了）跟零号机长得非常像~</p>
<p>庵野秀明不让我看新剧场版，我就去《环太平洋》里过瘾，哼 ╭(╯^╰)╮</p>
<p>还有，景甜竟然没有打酱油，还是个相当重要（混乱）的角色。但这不是她锅，是剧本的锅。</p>
<hr>
<p>时间也不早啦，就说到这里吧。明天还要苦逼的早起……</p>
</div>
        
        <div class="prism-action-bar">
            <div class="comment-action action-item"><a class="no-link text-uppercase" href="/archives/101/#prism__comment" target="_self"><i class="fa fa-comment"></i>评论</a></div>
        </div>
        
    </article>
    
    <article class="yue prism-post-item col-md-8 offset-md-2">
        <h1 class="prism-post-title"><a class="no-link" href="/archives/100/" target="_self">CUDA：矩阵乘法与瓦片化处理</a></h1>
        <div class="prism-post-time">
            <a class="no-link" href="/archives/100/" target="_self">
                <time class="text-uppercase">
                    March 25 2018
                </time>
            </a>
        </div>
        <div class="prism-content"><p>并行计算的另一个入门例子：矩阵乘法，附带粗略地说一说瓦片化处理的原因和在这个例子里的应用。</p>
<p>定义矩阵乘法：</p>
<center>$C_{m\times k}=A_{m\times n}\cdot B_{n\times k}$</center><p>其中</p>
<center>$C_{i,j}=\sum_{k=1}^nA_{i,k}\times B_{k,j}$</center><p>大矩阵乘法在 CPU 上的运算是很耗时的，其特点决定了它更适合并行处理。那么如何在 CUDA 上完成运算呢？</p>
<p><del>确实应该放图上来，但是实在是太懒不想做图……</del></p>
<h4>代码</h4>
<p>设两矩阵大小分别是 $m\times n$ 、$n\times k$ , <code>blocksize</code> 是一个 block 的大小，这个值是可以自己设定的，而且有不少学问，这里按下不表，暂且假设是一个小于 $m$ 和 $k$ 的数。这里略去分配显存和拷贝数据的过程，直接看 Device Code：</p>
<div class="highlight"><pre><span></span><span class="n">__global__</span> 
<span class="kt">void</span> <span class="nf">MatMUL</span><span class="p">(</span><span class="kt">int</span> <span class="n">m</span><span class="p">,</span><span class="kt">int</span> <span class="n">n</span><span class="p">,</span><span class="kt">int</span> <span class="n">k</span><span class="p">,</span><span class="kt">float</span> <span class="o">*</span><span class="n">A</span><span class="p">,</span><span class="kt">float</span> <span class="o">*</span><span class="n">B</span><span class="p">,</span><span class="kt">float</span> <span class="o">*</span><span class="n">C</span><span class="p">)</span> <span class="p">{</span>
    <span class="kt">int</span> <span class="n">Row</span> <span class="o">=</span> <span class="n">blockIdx</span><span class="p">.</span><span class="n">y</span><span class="o">*</span><span class="n">blockDim</span><span class="p">.</span><span class="n">y</span> <span class="o">+</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">y</span><span class="p">;</span>
    <span class="kt">int</span> <span class="n">Col</span> <span class="o">=</span> <span class="n">blockIdx</span><span class="p">.</span><span class="n">x</span><span class="o">*</span><span class="n">blockDim</span><span class="p">.</span><span class="n">x</span> <span class="o">+</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">;</span>

    <span class="k">if</span> <span class="p">((</span><span class="n">Row</span> <span class="o">&lt;</span> <span class="n">m</span><span class="p">)</span> <span class="o">&amp;&amp;</span> <span class="p">(</span><span class="n">Col</span> <span class="o">&lt;</span> <span class="n">k</span><span class="p">))</span> <span class="p">{</span>
        <span class="kt">float</span> <span class="n">c</span> <span class="o">=</span> <span class="mf">0.0</span><span class="p">;</span>
        <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">n</span><span class="p">;</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
            <span class="n">c</span> <span class="o">+=</span> <span class="n">A</span><span class="p">[</span><span class="n">Row</span><span class="o">*</span><span class="n">n</span> <span class="o">+</span> <span class="n">i</span><span class="p">]</span> <span class="o">*</span> <span class="n">B</span><span class="p">[</span><span class="n">i</span><span class="o">*</span><span class="n">k</span> <span class="o">+</span> <span class="n">Col</span><span class="p">];</span>
        <span class="p">}</span>
        <span class="n">C</span><span class="p">[</span><span class="n">k</span><span class="o">*</span><span class="n">Row</span> <span class="o">+</span> <span class="n">Col</span><span class="p">]</span> <span class="o">=</span> <span class="n">c</span><span class="p">;</span>
    <span class="p">}</span>
<span class="p">}</span>

<span class="n">__host__</span>
<span class="kt">int</span> <span class="nf">main</span><span class="p">(){</span>
    <span class="err">……</span>
    <span class="n">dim3</span> <span class="n">DimGrid</span><span class="p">(</span><span class="n">k</span> <span class="o">/</span> <span class="n">blocksize</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">m</span> <span class="o">/</span> <span class="n">blocksize</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">);</span>
    <span class="n">dim3</span> <span class="n">DimBlock</span><span class="p">(</span><span class="n">blocksize</span><span class="p">,</span> <span class="n">blocksize</span><span class="p">,</span> <span class="mi">1</span><span class="p">);</span>

    <span class="c1">//非瓦片化</span>
    <span class="n">MatMUL</span> <span class="o">&lt;&lt;&lt;</span><span class="n">DimGrid</span><span class="p">,</span> <span class="n">DimBlock</span> <span class="o">&gt;&gt;&gt;</span> <span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">d_A</span><span class="p">,</span> <span class="n">d_B</span><span class="p">,</span> <span class="n">d_C</span><span class="p">);</span>
    <span class="err">……</span>
<span class="p">}</span>
</pre></div>
<h4>简析</h4>
<p>从 <code>DimGrid</code> 与<code>DimBlock</code> 可知，这里的线程组织方式是二维的，定位某个线程的方法是：</p>
<div class="highlight"><pre><span></span><span class="kt">int</span> <span class="n">Row</span> <span class="o">=</span> <span class="n">blockIdx</span><span class="p">.</span><span class="n">y</span><span class="o">*</span><span class="n">blockDim</span><span class="p">.</span><span class="n">y</span> <span class="o">+</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">y</span><span class="p">;</span>
<span class="kt">int</span> <span class="n">Col</span> <span class="o">=</span> <span class="n">blockIdx</span><span class="p">.</span><span class="n">x</span><span class="o">*</span><span class="n">blockDim</span><span class="p">.</span><span class="n">x</span> <span class="o">+</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">;</span>
</pre></div>
<p>对应 $C$ 矩阵中的第 $Row$ 行 $Col$ 列的元素，其值即通过一个循环累加得到。注意虽然 A 与 B 矩阵是二维矩阵，但是它们的存储方式仍然是从上到下从左到右线性存储的，所以可以通过 <code>A[Row*n + i]</code> 这样的方式来访问矩阵元素值。</p>
<h4>瓦片化处理</h4>
<p>上面的代码中，GPU 执行过程中读写均发生在 global memory 上，但 global memory 速度慢于 block 内的 shared memory。瓦片化的思想是将某些频繁使用的值放进 shared memory 中，减少对 global memory 的访问次数，以此提高执行速度。但由于 shared memory 的大小有限，无法每次都把所有的数据载上去，因而每次只将 “一片” 元素值载入，故称「瓦片化处理」。</p>
<p><strong>代码</strong></p>
<div class="highlight"><pre><span></span><span class="n">__global__</span>
<span class="kt">void</span> <span class="nf">MatMUL_Tile</span><span class="p">(</span><span class="kt">int</span> <span class="n">m</span><span class="p">,</span> <span class="kt">int</span> <span class="n">n</span><span class="p">,</span> <span class="kt">int</span> <span class="n">k</span><span class="p">,</span> <span class="kt">float</span> <span class="o">*</span><span class="n">A</span><span class="p">,</span> <span class="kt">float</span> <span class="o">*</span><span class="n">B</span><span class="p">,</span> <span class="kt">float</span> <span class="o">*</span><span class="n">C</span><span class="p">)</span> <span class="p">{</span>
<span class="cp">#define T 7</span>
    <span class="n">__shared__</span> <span class="kt">float</span> <span class="n">ds_A</span><span class="p">[</span><span class="n">T</span><span class="p">][</span><span class="n">T</span><span class="p">];</span>
    <span class="n">__shared__</span> <span class="kt">float</span> <span class="n">ds_B</span><span class="p">[</span><span class="n">T</span><span class="p">][</span><span class="n">T</span><span class="p">];</span>

    <span class="kt">int</span> <span class="n">Row</span> <span class="o">=</span> <span class="n">blockIdx</span><span class="p">.</span><span class="n">y</span><span class="o">*</span><span class="n">T</span> <span class="o">+</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">y</span><span class="p">;</span>
    <span class="kt">int</span> <span class="n">Col</span> <span class="o">=</span> <span class="n">blockIdx</span><span class="p">.</span><span class="n">x</span><span class="o">*</span><span class="n">T</span> <span class="o">+</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">;</span>
    <span class="kt">int</span> <span class="n">tx</span> <span class="o">=</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">;</span>
    <span class="kt">int</span> <span class="n">ty</span> <span class="o">=</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">y</span><span class="p">;</span>

    <span class="kt">float</span> <span class="n">c</span> <span class="o">=</span> <span class="mf">0.0</span><span class="p">;</span>

    <span class="k">if</span> <span class="p">((</span><span class="n">Row</span> <span class="o">&lt;</span> <span class="n">m</span><span class="p">)</span> <span class="o">&amp;&amp;</span> <span class="p">(</span><span class="n">Col</span> <span class="o">&lt;</span> <span class="n">k</span><span class="p">))</span> 
    <span class="p">{</span>
        <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">t</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">t</span> <span class="o">&lt;</span> <span class="n">n</span> <span class="o">/</span> <span class="n">T</span> <span class="o">+</span> <span class="mi">1</span><span class="p">;</span> <span class="n">t</span><span class="o">++</span><span class="p">)</span>
        <span class="p">{</span>
            <span class="k">if</span> <span class="p">((</span><span class="n">Row</span> <span class="o">&lt;</span> <span class="n">m</span><span class="p">)</span> <span class="o">&amp;&amp;</span> <span class="p">(</span><span class="n">t</span><span class="o">*</span><span class="n">T</span> <span class="o">+</span> <span class="n">tx</span> <span class="o">&lt;</span> <span class="n">n</span><span class="p">))</span>    <span class="n">ds_A</span><span class="p">[</span><span class="n">ty</span><span class="p">][</span><span class="n">tx</span><span class="p">]</span> <span class="o">=</span> <span class="n">A</span><span class="p">[</span><span class="n">Row</span><span class="o">*</span><span class="n">n</span> <span class="o">+</span> <span class="n">t</span><span class="o">*</span><span class="n">T</span> <span class="o">+</span> <span class="n">tx</span><span class="p">];</span>
            <span class="k">else</span>                                <span class="n">ds_A</span><span class="p">[</span><span class="n">ty</span><span class="p">][</span><span class="n">tx</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
            <span class="k">if</span> <span class="p">((</span><span class="n">t</span><span class="o">*</span><span class="n">T</span> <span class="o">+</span> <span class="n">ty</span> <span class="o">&lt;</span> <span class="n">n</span><span class="p">)</span> <span class="o">&amp;&amp;</span> <span class="p">(</span><span class="n">Col</span> <span class="o">&lt;</span> <span class="n">k</span><span class="p">))</span>    <span class="n">ds_B</span><span class="p">[</span><span class="n">ty</span><span class="p">][</span><span class="n">tx</span><span class="p">]</span> <span class="o">=</span> <span class="n">B</span><span class="p">[(</span><span class="n">t</span><span class="o">*</span><span class="n">T</span> <span class="o">+</span> <span class="n">ty</span><span class="p">)</span><span class="o">*</span><span class="n">k</span> <span class="o">+</span> <span class="n">Col</span><span class="p">];</span>
            <span class="k">else</span>                                <span class="n">ds_B</span><span class="p">[</span><span class="n">ty</span><span class="p">][</span><span class="n">tx</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
            <span class="n">__syncthreads</span><span class="p">();</span>
            <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">T</span><span class="p">;</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span>
            <span class="p">{</span>
                <span class="n">c</span> <span class="o">+=</span> <span class="n">ds_A</span><span class="p">[</span><span class="n">ty</span><span class="p">][</span><span class="n">i</span><span class="p">]</span> <span class="o">*</span> <span class="n">ds_B</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">tx</span><span class="p">];</span>
            <span class="p">}</span>
            <span class="n">__syncthreads</span><span class="p">();</span>
        <span class="p">}</span>
        <span class="n">C</span><span class="p">[</span><span class="n">Row</span><span class="o">*</span><span class="n">k</span> <span class="o">+</span> <span class="n">Col</span><span class="p">]</span> <span class="o">=</span> <span class="n">c</span><span class="p">;</span>
    <span class="p">}</span>
<span class="p">}</span>
</pre></div>
<p>T 代表每个瓦片的尺寸，它与 block 的大小一致。<code>__syncthreads();</code> 的作用是让 block 内的线程保持一样的执行进度，在遍历过程中瓦片中的值会被后面的操作覆盖，同步所有的线程可以防止读取、写入混乱。</p>
<p>要特别注意边界条件，保证线程覆盖所有的待计算元素。当瓦片大小与矩阵大小不成整数比时，为覆盖所有元素，瓦片势必有一部分在矩阵之外，这部分瓦片的元素应该赋值为 0，否则可能发生意想不到的后果。</p>
</div>
        
        <div class="prism-action-bar">
            <div class="comment-action action-item"><a class="no-link text-uppercase" href="/archives/100/#prism__comment" target="_self"><i class="fa fa-comment"></i>评论</a></div>
        </div>
        
    </article>
    
    <article class="yue prism-post-item col-md-8 offset-md-2">
        <h1 class="prism-post-title"><a class="no-link" href="/archives/99/" target="_self">Hello CUDA！第一个 CUDA 程序</a></h1>
        <div class="prism-post-time">
            <a class="no-link" href="/archives/99/" target="_self">
                <time class="text-uppercase">
                    March 21 2018
                </time>
            </a>
        </div>
        <div class="prism-content"><p>最近要和 CUDA 打交道，作为笔记，就来一个 Hello World for CUDA 吧！</p>
<p>Compute Unified Device Architecture（CUDA）是英伟达为旗下图形硬件开发的一套并行计算平台，目前已经到 CUDA 9 了。这个平台提供了简洁易用的 API，可以让使用者以类 C 的语法来进行存储管理、进行计算任务。作为 CUDA 的 Hello World，这里就来讲解一个简单的 CUDA 程序：一维向量加，以及一个基本的 CUDA 程序需要包括哪些部分。</p>
<blockquote><p>这里略过 CUDA 的配置过程。本文使用的环境是 CUDA 9.1 + VS2015</p>
</blockquote>
<h3>代码</h3>
<p>首先先把代码扔上来：</p>
<div class="highlight"><pre><span></span><span class="cp">#include</span> <span class="cpf">&lt;iostream&gt;</span><span class="cp"></span>
<span class="cp">#include</span> <span class="cpf">&lt;device_launch_parameters.h&gt;</span><span class="cp"></span>
<span class="cp">#include</span> <span class="cpf">&lt;cuda_runtime.h&gt;</span><span class="cp"></span>

<span class="k">using</span> <span class="k">namespace</span> <span class="n">std</span><span class="p">;</span>

<span class="n">__global__</span> <span class="c1">//这里是要在显卡上运行的代码，用 __global__ 来指明</span>
<span class="kt">void</span> <span class="n">VecAdd</span><span class="p">(</span><span class="kt">float</span> <span class="o">*</span><span class="n">d_A</span><span class="p">,</span> <span class="kt">float</span> <span class="o">*</span><span class="n">d_B</span><span class="p">,</span> <span class="kt">float</span> <span class="o">*</span><span class="n">d_C</span><span class="p">)</span> <span class="p">{</span>
    <span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">;</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">i</span><span class="o">&lt;</span><span class="mi">512</span><span class="p">)</span> <span class="n">d_C</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">d_A</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">+</span> <span class="n">d_B</span><span class="p">[</span><span class="n">i</span><span class="p">];</span>
<span class="p">}</span>

<span class="n">__host__</span> <span class="c1">//这里是在 CPU 上运行的代码，用 __host__ 来指明</span>
<span class="kt">int</span> <span class="n">main</span><span class="p">()</span> <span class="p">{</span>
    <span class="kt">float</span> <span class="n">h_A</span><span class="p">[</span><span class="mi">512</span><span class="p">],</span> <span class="n">h_B</span><span class="p">[</span><span class="mi">512</span><span class="p">],</span> <span class="n">h_C</span><span class="p">[</span><span class="mi">512</span><span class="p">];</span>

    <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="mi">512</span><span class="p">;</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
        <span class="n">h_A</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">i</span><span class="p">;</span>
        <span class="n">h_B</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="mi">3</span><span class="p">;</span>
    <span class="p">}</span>

    <span class="kt">float</span> <span class="o">*</span><span class="n">d_A</span><span class="p">,</span> <span class="o">*</span><span class="n">d_B</span><span class="p">,</span> <span class="o">*</span><span class="n">d_C</span><span class="p">;</span>

    <span class="c1">//在显卡上分配内存</span>
    <span class="n">cudaMalloc</span><span class="p">((</span><span class="kt">void</span><span class="o">**</span><span class="p">)</span><span class="o">&amp;</span><span class="n">d_A</span><span class="p">,</span> <span class="k">sizeof</span><span class="p">(</span><span class="kt">float</span><span class="p">)</span> <span class="o">*</span> <span class="mi">512</span><span class="p">);</span> 
    <span class="n">cudaMalloc</span><span class="p">((</span><span class="kt">void</span><span class="o">**</span><span class="p">)</span><span class="o">&amp;</span><span class="n">d_B</span><span class="p">,</span> <span class="k">sizeof</span><span class="p">(</span><span class="kt">float</span><span class="p">)</span> <span class="o">*</span> <span class="mi">512</span><span class="p">);</span>
    <span class="n">cudaMalloc</span><span class="p">((</span><span class="kt">void</span><span class="o">**</span><span class="p">)</span><span class="o">&amp;</span><span class="n">d_C</span><span class="p">,</span> <span class="k">sizeof</span><span class="p">(</span><span class="kt">float</span><span class="p">)</span> <span class="o">*</span> <span class="mi">512</span><span class="p">);</span>

    <span class="c1">//将内存中的数据拷贝到显卡上</span>
    <span class="n">cudaMemcpy</span><span class="p">(</span><span class="n">d_A</span><span class="p">,</span> <span class="n">h_A</span><span class="p">,</span> <span class="k">sizeof</span><span class="p">(</span><span class="n">h_A</span><span class="p">),</span> <span class="n">cudaMemcpyHostToDevice</span><span class="p">);</span>
    <span class="n">cudaMemcpy</span><span class="p">(</span><span class="n">d_B</span><span class="p">,</span> <span class="n">h_B</span><span class="p">,</span> <span class="k">sizeof</span><span class="p">(</span><span class="n">h_B</span><span class="p">),</span> <span class="n">cudaMemcpyHostToDevice</span><span class="p">);</span>

    <span class="c1">//这俩参数用来决定线程的组织方式，看下文</span>
    <span class="n">dim3</span> <span class="nf">DimGrid</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">);</span>
    <span class="n">dim3</span> <span class="nf">DimBlock</span><span class="p">(</span><span class="mi">512</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">);</span>

    <span class="c1">//在显卡上执行代码</span>
    <span class="n">VecAdd</span> <span class="o">&lt;&lt;&lt;</span><span class="n">DimGrid</span><span class="p">,</span> <span class="n">DimBlock</span> <span class="o">&gt;&gt;&gt;</span><span class="p">(</span><span class="n">d_A</span><span class="p">,</span> <span class="n">d_B</span><span class="p">,</span> <span class="n">d_C</span><span class="p">);</span>

    <span class="c1">//将运算结果从显卡拷贝回来</span>
    <span class="n">cudaMemcpy</span><span class="p">(</span><span class="n">h_C</span><span class="p">,</span> <span class="n">d_C</span><span class="p">,</span> <span class="k">sizeof</span><span class="p">(</span><span class="n">h_C</span><span class="p">),</span> <span class="n">cudaMemcpyDeviceToHost</span><span class="p">);</span>

    <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="mi">512</span><span class="p">;</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
        <span class="n">cout</span> <span class="o">&lt;&lt;</span> <span class="n">h_C</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">&lt;&lt;</span> <span class="sc">&#39; &#39;</span><span class="p">;</span>
    <span class="p">}</span>

    <span class="c1">//释放显存</span>
    <span class="n">cudaFree</span><span class="p">(</span><span class="n">d_A</span><span class="p">);</span>
    <span class="n">cudaFree</span><span class="p">(</span><span class="n">d_B</span><span class="p">);</span>
    <span class="n">cudaFree</span><span class="p">(</span><span class="n">d_C</span><span class="p">);</span>

    <span class="k">return</span> <span class="mi">0</span><span class="p">;</span>
<span class="p">}</span>
</pre></div>
<h3>简单解析</h3>
<p>像上面一个最简单的 CUDA 程序总结起来可以有以下几个部分</p>
<p><strong>宿主机代码</strong></p>
<p>所谓宿主机代码（host code），可理解为执行在 CPU 上的代码。这部分代码分为几块的话大概是下面这样：</p>
<ul>
<li>在 CPU 上分配内存，确定需要处理的数据</li>
<li>在显卡上分配内存</li>
<li>将内存中的数据拷贝到显卡上</li>
<li>决定线程组织方式</li>
<li>执行 kernel 代码（Device 代码）</li>
<li>将结果拷贝回 CPU</li>
<li>释放显存</li>
</ul>
<p>大部分都没什么可说的，重点在「决定线程组织方式」上。可以这么说：显卡之所以适用于并行计算，正是因为它允许同一时刻有极大量的线程在执行。the trick is，记住让每个线程<em>进行同样的任务，只是处理不同的数据</em>。对一个一维向量加，有 C[i]=A[i]+B[i]，也就是说，每个线程都做加法，但是由 i 来决定要操作的两个数（A[i]与B[i]）以及结果的存放位置（C[i]）。这样一来，一个 512 维的向量加，让 512 个线程来执行就好啦。</p>
<p>所谓的 i 来决定处理不同的数据，在 CUDA 中也就是告诉显卡每个线程要操作什么数据。这与 CUDA 中线程的组织方式很有关系。线程是多线程处理的最小单位，将线程以一维或者二维或者三维组织起来就称为一个「block」，再将 block 以一维或者二维组织起来则成为一个「grid」，一个 grid 也就是一个「kernel」。宿主机代码调用的即是 kernel。看上面代码中的这几行：</p>
<div class="highlight"><pre><span></span><span class="c1">//这俩参数用来决定线程的组织方式，看下文</span>
<span class="n">dim3</span> <span class="nf">DimGrid</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">);</span>
<span class="n">dim3</span> <span class="nf">DimBlock</span><span class="p">(</span><span class="mi">512</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">);</span>
</pre></div>
<p>可见用 dim3 的类型声明了两个变量 <code>DimGrid</code> 、<code>DimBlock</code> ，其中以 <code>1,1,1</code> 的方式组织 grid，也就是说，这个 gird 中包含一个 block（x、y、z 方向上都是 1）。以<code>512,1,1</code> 的方式组织 block 中的 thread，也就是说这个 block 中包含 512 个 thread 排成一行（x 方向上是 512，y、z 方向上都是 1）。</p>
<p>决定线程组织方式时要注意能够覆盖所有的待处理数据，在这里向量长 512 ，因此总的线程数不能低于 512。注意，由于 grid 只能以一维或者二维组织，因此 DimGrid 第三个参数总是 1。决定了线程的组织方式后就需要调用 kernel 代码执行真正的操作。</p>
<p><strong>Device 代码</strong></p>
<div class="highlight"><pre><span></span><span class="c1">//在显卡上执行代码</span>
<span class="n">VecAdd</span> <span class="o">&lt;&lt;&lt;</span><span class="n">DimGrid</span><span class="p">,</span> <span class="n">DimBlock</span> <span class="o">&gt;&gt;&gt;</span><span class="p">(</span><span class="n">d_A</span><span class="p">,</span> <span class="n">d_B</span><span class="p">,</span> <span class="n">d_C</span><span class="p">);</span>
</pre></div>
<p>这行代码调用了执行在显卡上的函数 <code>VecAdd</code>，带上了三个指针以及 <code>DimGrid</code> 和 <code>DimBlock</code> 。于是显卡便根据此分配线程。于是在每个线程上将执行以下代码：</p>
<div class="highlight"><pre><span></span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">;</span>
<span class="k">if</span> <span class="p">(</span><span class="n">i</span><span class="o">&lt;</span><span class="mi">512</span><span class="p">)</span> <span class="n">d_C</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">d_A</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">+</span> <span class="n">d_B</span><span class="p">[</span><span class="n">i</span><span class="p">];</span>
</pre></div>
<p>首先通过<code>threadIdx.x</code> 决定 i，这里的 i 将介于 0 至 511 之间。也就是说，i 号线程执行的操作是：<code>d_C[i] = d_A[i] + d_B[i];</code> 。这个简单的程序就是这样。在这个例子里只用到了 <code>threadIdx</code> ，因为线程组织方式是一维的。对更复杂的线程组织方式，可能需要更多的参数，例如 <code>blockIdx</code>。</p>
<p>记住，the trick is：<strong>记住让每个线程进行同样的任务，只是处理不同的数据</strong> 。如何实现这一点？每个线程有不同的线程 id，通过不同的线程 id 即可达成不同的线程执行一样的代码，但是处理不同的数据。</p>
<h3>其他</h3>
<p>以上是我的个人理解，而且我也刚开始学，可能有很多不准确的地方。关于 GPU 中的存储管理的部分我略过了，因为我其实并不那么明白……</p>
</div>
        
        <div class="prism-action-bar">
            <div class="comment-action action-item"><a class="no-link text-uppercase" href="/archives/99/#prism__comment" target="_self"><i class="fa fa-comment"></i>评论</a></div>
        </div>
        
    </article>
    
</section>

<div class="container">
    <section id="prism__page__pagination" class="prism-pagination" class="col-md-8 offset-md-2">
        <ul>
            
            <li class="next">
                <a class="no-link" href="/page/6/" target="_self"><i class="fa fa-chevron-left" aria-hidden="true"></i>更新</a>
            </li>
            
            
            <li class="prev">
                <a class="no-link" href="/page/8/" target="_self">更旧<i class="fa fa-chevron-right" aria-hidden="true"></i></a>
            </li>
            
        </ul>
    </section>
</div>


</main>

            <footer id="prism__footer">
                <section>
                    <div>
                        <nav class="social-links">
                            <ul><li><a class="no-link" title="Twitter" href="https://twitter.com/AlanDecode" target="_blank" rel="noopener noreferrer nofollow"><i class="gi gi-twitter"></i></a></li><li><a class="no-link" title="GitHub" href="https://github.com/AlanDecode" target="_blank" rel="noopener noreferrer nofollow"><i class="gi gi-github"></i></a></li><li><a class="no-link" title="Weibo" href="https://weibo.com/5245109677/" target="_blank" rel="noopener noreferrer nofollow"><i class="gi gi-weibo"></i></a></li></ul>
                        </nav>
                    </div>

                    <section id="prism__external_links">
                        <ul>
                            
                            <li>
                                <a class="no-link" target="_blank" href="https://github.com/AlanDecode/Maverick" rel="noopener noreferrer nofollow">Maverick</a>：🏄‍ Go My Own Way.
                                <span>|</span>
                            </li>
                            
                            <li>
                                <a class="no-link" target="_blank" href="https://www.imalan.cn" rel="noopener noreferrer nofollow">三無計劃</a>：三是虚指。至于是哪三无，我唔知。
                                <span>|</span>
                            </li>
                            
                            <li>
                                <a class="no-link" target="_blank" href="https://wiki.imalan.cn" rel="noopener noreferrer nofollow">無知識</a>：熊猫小A的Wiki站点。隶属于「三无计划」。
                                <span>|</span>
                            </li>
                            
                            <li>
                                <a class="no-link" target="_blank" href="https://lab.imalan.cn" rel="noopener noreferrer nofollow">無項目</a>：熊猫小A的实验室。隶属于「三无计划」。
                                <span>|</span>
                            </li>
                            
                            <li>
                                <a class="no-link" target="_blank" href="https://t.me/triple_null" rel="noopener noreferrer nofollow">無消息</a>：熊猫小A的广播。隶属于「三无计划」。
                                <span>|</span>
                            </li>
                            
                        </ul>
                    </section>

                    <div class="copyright">
                        <p class="copyright-text">
                            <span class="brand">无文字 | 三无计划</span>
                            <span>Copyright © 2020 熊猫小A</span>
                        </p>
                        <p class="copyright-text powered-by">
                            | Powered by <a href="https://github.com/AlanDecode/Maverick" class="no-link" target="_blank" rel="noopener noreferrer nofollow">Maverick</a> | Theme <a href="https://github.com/Reedo0910/Maverick-Theme-Prism" target="_blank" class="no-link" rel="noopener noreferrer nofollow">Prism</a>
                        </p>
                    </div>
                    <div class="footer-addon">
                        
<a no-style href="http://beian.miit.gov.cn" target="_blank">京ICP备18000133号-1</a> | 
<a no-style href="https://www.upyun.com" target="_blank">又拍云</a>

                    </div>
                </section>
                <script>
                    var site_build_date = "2017-06-29T12:00+08:00"

                </script>
                <script src="https://cdn.jsdelivr.net/gh/AlanDecode/site-Blog@gh-pages/assets/prism-a1bd79b1a4.js"></script>
            </footer>
        </div>
    </div>
    </div>

    <script src="https://cdn.jsdelivr.net/gh/AlanDecode/site-Blog@gh-pages/assets/ExSearch/jquery.min.js"></script>
    <script src="https://cdn.jsdelivr.net/gh/AlanDecode/site-Blog@gh-pages/assets/ExSearch/ExSearch-493cb9cd89.js"></script>

    <!--katex-->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/AlanDecode/site-Blog@gh-pages/assets/katex.min.css">
    <script defer src="https://cdn.jsdelivr.net/gh/AlanDecode/site-Blog@gh-pages/assets/katex.min.js"></script>
    <script>
        mathOpts = {
            delimiters: [
                { left: "$$", right: "$$", display: true },
                { left: "\\[", right: "\\]", display: true },
                { left: "$", right: "$", display: false },
                { left: "\\(", right: "\\)", display: false }
            ]
        };

    </script>
    <script defer src="https://cdn.jsdelivr.net/gh/AlanDecode/site-Blog@gh-pages/assets/auto-render.min.js" onload="renderMathInElement(document.body, mathOpts);"></script>

    
<script>
    var _hmt = _hmt || [];
    (function() {
    var hm = document.createElement("script");
    hm.src = "https://hm.baidu.com/hm.js?e4f3a7c02ac2aabc41a1cfa95f61a026";
    var s = document.getElementsByTagName("script")[0]; 
    s.parentNode.insertBefore(hm, s);
    })();
</script>
<script>
    (function(){
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        }
        else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>
<script>
if(window.location.hash){
    var checkExist = setInterval(function() {
       if ($(window.location.hash).length) {
          $('html, body').animate({scrollTop: $(window.location.hash).offset().top-90}, 1000);
          clearInterval(checkExist);
       }
    }, 100);
}
</script>
<script>
if(window.navigator && navigator.serviceWorker) {
  caches.keys().then(function(cacheNames) {
    cacheNames.forEach(function(cacheName) {
      caches.delete(cacheName);
    });
  }).then(function(){
    console.log('Cache cleaned.');
  });
  navigator.serviceWorker.getRegistrations()
  .then(function(registrations) {
    for(let registration of registrations) {
      registration.unregister();
    }
  }).then(function(){
    console.log('Service Worker stopped.');
  });
}
</script>

</body>

</html>